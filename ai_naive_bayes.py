# -*- coding: utf-8 -*-
"""AI Naive Bayes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ixO4Q7dk3VJ0axzGdnbLOyryjG6vExt-

1. import libraries and datasets
"""

import re
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
import math
import nltk
from sklearn.feature_extraction.text import CountVectorizer
from collections import defaultdict

data = pd.read_excel('byd_reviews.xlsx')
data

"""2. Data Pre-processing"""

def remove_tags(string):
    # Ensure string is not None or non-string type
    if not isinstance(string, str):
        return ""
    # Remove HTML tags (corrected regex)
    result = re.sub(r'<.*?>', '', string)
    # Remove URLs (more robust regex)
    result = re.sub(r'https?://\S+|www\.\S+', '', result)
    # Remove non-alphanumeric characters, keep spaces
    result = re.sub(r'[^a-zA-Z0-9\s]', ' ', result)
    # Convert to lowercase
    result = result.lower()
    return result
data['review_text']=data['review_text'].apply(lambda cw : remove_tags(cw))
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
data['review_text'] = data['review_text'].apply(lambda x: ' '.join([word for word in x.split()
                      if word not in (stop_words)]))

data

print(data['review_text'].head())
print(type(data['review_text'].iloc[0]))

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('punkt_tab')
nltk.download('wordnet') # Added this line to download the wordnet corpus

lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    if not isinstance(text, str):
        return ""
    tokens = word_tokenize(text)
    return " ".join([lemmatizer.lemmatize(w) for w in tokens])

data['review_text'] = data['review_text'].apply(lemmatize_text)

lemmatize_text("This product works very well")

w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()

def lemmatize_text(text):
    st = ""
    for w in w_tokenizer.tokenize(text):
        st = st + lemmatizer.lemmatize(w) + " "
    return st.strip()

data['review_text'] = data['review_text'].apply(lemmatize_text)

data

"""3. Encoding labels"""

reviews = data['review_text'].values
labels = data['sentiment'].values
encoder = LabelEncoder()
encoded_labels = encoder.fit_transform(labels)

"""4. Train test split"""

train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels, stratify = encoded_labels)

train_sentences.shape, test_sentences.shape

train_labels.shape, test_labels.shape

"""5. build using formulas"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

vectorizer = TfidfVectorizer(
    lowercase=True,
    stop_words='english',
    max_features=5000
)

X_train = vectorizer.fit_transform(train_sentences)
X_test = vectorizer.transform(test_sentences)

nb_model = MultinomialNB()
nb_model.fit(X_train, train_labels)

y_pred = nb_model.predict(X_test)

print("Accuracy:", accuracy_score(test_labels, y_pred))
print(classification_report(test_labels, y_pred, target_names=encoder.classes_))

"""6. Fit models"""

labels = [0,1,2]
n_label_items, log_label_priors = fit(train_sentences,train_labels,labels)
pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_sentences)
print("Accuracy of prediction on test set : ", accuracy_score(test_labels,pred))

data

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Generate the confusion matrix
conf_matrix = confusion_matrix(test_labels, y_pred)

# Get the actual sentiment labels (e.g., 'negative', 'neutral', 'positive')
# Assuming `encoder` object is available from cell `lF-gyGFttSUa`
sentiment_labels = encoder.inverse_transform(sorted(list(set(test_labels))))

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=sentiment_labels, yticklabels=sentiment_labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt
import pandas as pd # Import pandas

# Get the actual sentiment labels (e.g., 'negative', 'neutral', 'positive')
# Assuming 'encoder' and 'test_labels' are available from previous cells.
# This line is crucial for meaningful labels on the plot.
sentiment_labels = encoder.inverse_transform(sorted(list(set(test_labels))))

# Convert the confusion matrix (numpy array) to a pandas DataFrame for easier plotting
conf_df = pd.DataFrame(conf_matrix, index=sentiment_labels, columns=sentiment_labels)

# Plot stacked bar chart using the DataFrame's plot method
conf_df.plot(kind='bar', stacked=True, figsize=(8,6), colormap='viridis')

plt.title("True vs Predicted Sentiment Labels")
plt.xlabel("True Labels")
plt.ylabel("Number of Reviews")
plt.legend(title='Predicted Labels')
plt.xticks(rotation=0) # Keep x-axis labels horizontal
plt.show()

reviews

predicted_sentiments = encoder.inverse_transform(pred)

# Create a DataFrame to display the review text and its predicted sentiment
results_df = pd.DataFrame({
    'Review Text': test_sentences,
    'Predicted Sentiment': predicted_sentiments
})

print(results_df.head())
print(f"\nTotal predictions shown: {len(results_df)}")

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# --- Define preprocessing functions locally for this cell to ensure consistency ---
# The `remove_tags` function from cell -W4l34hWrPJH
def remove_tags_for_prediction(string):
    if not isinstance(string, str):
        return ""
    result = re.sub(r'<.*?>', '', string)
    result = re.sub(r'https?://\S+|www\.\S+', '', result)
    result = re.sub(r'[^a-zA-Z0-9\s]', ' ', result)
    result = result.lower()
    return result

# Download stopwords and wordnet if not already done in the session (quiet=True to suppress output)
nltk.download('stopwords', quiet=True)
stop_words = set(stopwords.words('english'))

# Use the same tokenizer and lemmatizer as in the training pipeline (from 9itS3fTlrswR)
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
nltk.download('wordnet', quiet=True)
lemmatizer = WordNetLemmatizer()

# This is the `lemmatize_text` function from cell 9itS3fTlrswR
def lemmatize_text_for_prediction(text):
    st = ""
    for w in w_tokenizer.tokenize(text):
        st = st + lemmatizer.lemmatize(w) + " "
    return st.strip()


def predict_user_sentiment(user_sentence):
    # 1. Preprocess the user's input sentence using the same steps as training
    cleaned_sentence = remove_tags_for_prediction(user_sentence)
    stopword_removed_sentence = ' '.join([word for word in cleaned_sentence.split() if word not in (stop_words)])
    lemmatized_sentence = lemmatize_text_for_prediction(stopword_removed_sentence)

    # The custom predict function expects a list of sentences
    processed_sentence_list = [lemmatized_sentence]

    # 2. Use the custom 'predict' function (defined in HPqCEyAtEHsc)
    # It requires: n_label_items, vocab, word_counts, log_label_priors, labels, and the text list.
    # These variables are assumed to be available from previously executed cells.
    predicted_encoded = predict(n_label_items, vocab, word_counts, log_label_priors, labels, processed_sentence_list)

    # 3. Inverse transform the encoded prediction (e.g., 0, 1, 2) to the original sentiment string ('negative', 'neutral', 'positive')
    predicted_sentiment_label = encoder.inverse_transform(predicted_encoded)[0]

    return predicted_sentiment_label

# --- Get user input and predict sentiment ---
user_input_sentence = input("Enter a sentence to predict its sentiment: ")

sentiment = predict_user_sentiment(user_input_sentence)
print(f"The predicted sentiment for '{user_input_sentence}' is: {sentiment}")

"""7. LLM"""

pip install ollama

import ollama

def explain_negative_review(review_text):
    prompt = f"""
    You are an AI analyst helping an EV company improve its products.

    The following customer review is classified as NEGATIVE.

    Review:
    "{review_text}"

    Task:
    - Explain clearly WHY the review is negative
    - Identify the main issue (e.g. battery, price, charging, performance, service)
    - Keep explanation concise and professional
    """

    response = ollama.chat(
        model="llama3.2",
        messages=[{"role": "user", "content": prompt}]
    )

    return response["message"]["content"]

negative_reviews = data[data['sentiment'] == 'negative']

negative_reviews['justification'] = negative_reviews['review_text'].apply(
    explain_negative_review
)